{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0f5c90-a1c3-44bd-881f-0df6722cfc10",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Tokenize text\n",
    "* Investigate more model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c688ce0f-a419-4b50-8b75-cf046258a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7590d9af-6fea-4a32-bda4-07c050dfd9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e749152f-0bfc-4d9d-8a60-5663f9dfc2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 17:05:35.416192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-22 17:05:35.429579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-22 17:05:35.429708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(gpus)\n",
    "tf.config.set_logical_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.LogicalDeviceConfiguration(memory_limit=8000)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2954cc6d-42db-4a5e-a045-d9c96e4b8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"./data/robert_frost.txt\").read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e049c87-0028-4b12-8672-00456efae82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=3000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t', lower=True, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts([text])\n",
    "sequences = tokenizer.texts_to_sequences([text])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=55, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e7e61a1-ee32-4c81-a9af-52acacbf8bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24c38c75-47f2-46bc-b16e-106d306412c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"the pasture\\n\\n\\ni'm going out to clean the pasture spring \\ni'll only stop to rake the leaves away\\n and wait to\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([sequences[0][:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d78565-e71c-4266-8a81-f3ad87bcf52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 8\n",
    "step = 5\n",
    "\n",
    "sentence = []\n",
    "next_char = []\n",
    "\n",
    "for i in range(0, len(sequences[0]) - max_length, step):\n",
    "    sentence.append(sequences[0][i : i + max_length])\n",
    "    next_char.append(sequences[0][i + max_length])\n",
    "\n",
    "\n",
    "X, Y = np.array(sentence), keras.utils.to_categorical(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a07c8101-4109-41eb-abd7-b4ca365a4ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4275, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bafed8c-8f3b-4e66-b432-4c270a3186d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38017\n"
     ]
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "layers = [\n",
    "    Embedding(tokenizer.num_words, 1024),\n",
    "    # Bidirectional(LSTM(2048, return_sequences=True)),\n",
    "    # Bidirectional(LSTM(2048, return_sequences=True)),\n",
    "    # Bidirectional(LSTM(1024)),\n",
    "    LSTM(1024),\n",
    "    Dense(tokenizer.num_words, activation=\"softmax\"),\n",
    "]\n",
    "\n",
    "model = tf.keras.Sequential(layers)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d88d6fa-e418-469d-a128-720fe94189e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 1024)        3072000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1024)              8392704   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3000)              3075000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14539704 (55.46 MB)\n",
      "Trainable params: 14539704 (55.46 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bff523b5-a91e-432e-9ead-7f4a0cc69ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27bb24b8-a7ff-4855-908f-954978af0bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "107/107 [==============================] - 5s 29ms/step - loss: 6.8738 - val_loss: 6.1040\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 6.0214 - val_loss: 6.1395\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 5.7098 - val_loss: 6.2184\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 5.2503 - val_loss: 6.5527\n",
      "Epoch 5/20\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 4.5700 - val_loss: 6.8847\n",
      "Epoch 6/20\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 3.6449 - val_loss: 7.3382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x732705e203d0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, validation_split=0.2, epochs=20, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34907650-be39-4d37-aee7-4afd31d9fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"./models/model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f35fb69-53e3-4040-9e84-455049c040ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 1456,  109,   26,    3,  931,    2,  232],\n",
       "       [ 931,    2,  232,  394,  233,  114,  395,    3],\n",
       "       [ 114,  395,    3,  932,    2,  209, 1457,    6],\n",
       "       [ 209, 1457,    6,  449,    3,  450,    2,  194],\n",
       "       [ 450,    2,  194,  504,    4,  132,   27,  396],\n",
       "       [ 132,   27,  396,   29,  123,  124,   79,    7],\n",
       "       [ 124,   79,    7,   59,   95, 1458,  109,   26],\n",
       "       [1458,  109,   26,    3,  701,    2,   96, 1459],\n",
       "       [   2,   96, 1459,  451,   64,    2,  397,   62],\n",
       "       [   2,  397,   62,   38,  254,  125, 1460,   56]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5b16c730-ef1d-404b-9fa7-c4f37046463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas), preds\n",
    "\n",
    "def genText(seq, tokenizer: Tokenizer, number_gen=200, temp=1):\n",
    "    generated = seq\n",
    "    sent = seq\n",
    "    text_in = tokenizer.sequences_to_texts(seq)\n",
    "    # print(text_in, end='')\n",
    "    \n",
    "    for _ in range(number_gen):\n",
    "        print(sent[0])\n",
    "        t = list()\n",
    "        \n",
    "        pred = model.predict(sent, verbose=0)[0]\n",
    "        # next_index, _  = sample(pred, temp)\n",
    "        next_index = np.argmax(pred)\n",
    "        next_char = tokenizer.sequences_to_texts([[next_index]])\n",
    "        \n",
    "        generated += [next_index]\n",
    "        sent = sent[0][1:] + [next_index]\n",
    "        t.append(sent)\n",
    "        t = np.array(t)\n",
    "        sent = t.copy()\n",
    "        \n",
    "        sys.stdout.write(next_char[0])\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d2aaf991-43d1-44a3-b1c7-ffdf7d79b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89, 99, 3, 34, 9, 8, 305, 12]\n",
      "the[ 99   3  34   9   8 305  12   2]\n",
      "the[  5  36  11  10 307  14   4]\n",
      "the[ 38  13  12 309  16   6]\n",
      "the[ 15  14 311  18   8]\n",
      "the[ 16 313  20  10]\n",
      "the[315  22  12]\n",
      "the[24 14]\n",
      "the[16]\n",
      "the[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 18:02:35.253223: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at cudnn_rnn_ops.cc:1771 : INVALID_ARGUMENT: max_seq_length <= 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node CudnnRNN defined at (most recent call last):\n<stack traces unavailable>\nmax_seq_length <= 0\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/lstm/PartitionedCall]] [Op:__inference_predict_function_58807]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text2gen \u001b[38;5;241m=\u001b[39m [sequences[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2000\u001b[39m:\u001b[38;5;241m2008\u001b[39m]]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext2gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[159], line 19\u001b[0m, in \u001b[0;36mgenText\u001b[0;34m(seq, tokenizer, number_gen, temp)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(sent[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     17\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m---> 19\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# next_index, _  = sample(pred, temp)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m next_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(pred)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu11_cv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu11_cv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node CudnnRNN defined at (most recent call last):\n<stack traces unavailable>\nmax_seq_length <= 0\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/lstm/PartitionedCall]] [Op:__inference_predict_function_58807]"
     ]
    }
   ],
   "source": [
    "text2gen = [sequences[0][2000:2008]]\n",
    "genText(text2gen, tokenizer, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bc808-b746-47d3-99b3-c2f95a2cd17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec59c0-1043-4375-b251-daa80069d92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
